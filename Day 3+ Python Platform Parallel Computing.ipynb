{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Alt-текст](Images/LSA_Header_3TROk.png) \n",
    "<p style=\"text-align: center; font-size: 36pt; font-weight: bold; color: rgb(0, 0, 190); padding: 20px; margin-top: 2%;\">ТРЕК</p>\n",
    "<p style=\"text-align: center; font-size: 36pt; font-weight: bold; color: rgb(0, 0, 190); padding: 40px; margin-top: -3%; line-height: 100%;\">Применение платформы Python для высокопроизводительных вычислений</p>\n",
    "<p style=\"text-align: left; font-size: 20pt; font-weight: bold; color: rgb(0, 0, 190); padding: 20px; margin-top: -2%; line-height: 10%;\">Руководитель:</p>\n",
    "<p style=\"text-align: left; font-size: 20pt; font-weight: bold; color: rgb(0, 0, 190); padding: 18px; margin-top: -2%; line-height: 100%;\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Русол Андрей Владимирович, к.т.н., с.н.с. ГЕОХИ РАН</p>\n",
    "<p style=\"text-align: left; font-size: 16pt; font-weight: bold; color: rgb(0, 0, 190); padding: 2px; margin-top: 2%;margin-left: 7%;\">E-Mail: fermata@inbox.ru</p>\n",
    "<p style=\"text-align: center; font-size: 16pt; font-weight: bold; color: rgb(0, 0, 190); padding: 20px; margin-top: 2%;\">Москва 2018</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"text-align: center; font-size: 36pt; font-weight: bold; color: rgb(0, 0, 190); padding: 20px; margin-top: 2%;\">ДЕНЬ ТРЕТИЙ И ДАЛЕЕ</p>\n",
    "<p style=\"text-align: center; font-size: 36pt; font-weight: bold; color: rgb(0, 0, 190); padding: 40px; margin-top: 3%; line-height: 150%;\">Технологии параллельного программирования на платформе Python: пакеты IPyparallel, MPI4Py и др.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "С появлением многоядерных процессоров параллельная реализация алгоритмов стала общеупотребительной практикой прикладного программирования, распространяющей нагрузку на все доступные ядра. В настоящее время сформировались четыре основных архитектуры параллельного программирования:\n",
    "- Single instruction, single data (SISD)\n",
    "- Single instruction, multiple data (SIMD)\n",
    "- Multiple instruction, single data (MISD)\n",
    "- Multiple instruction, multiple data (MIMD)\n",
    "\n",
    "#### Архитектура **SISD**\n",
    "\n",
    "Вычислительная система в архитектуре SISD является однопроцессорной машиной. Она выполняет одну команду, которая работает с одним потоком данных. В SISD машинные инструкции обрабатываются последовательно.\n",
    "\n",
    "В цикле синхронизации CPU выполняет следующие операции:\n",
    "\n",
    "#### Архитектура **МISD**\n",
    "\n",
    "В этой модели **N** процессоров, каждый со своим блоком управления, совместно используют **ОДИН** блок памяти. В каждом такте данные, полученные из памяти, обрабатываются всеми процессорами одновременно, каждый в соответствии с инструкциями, полученными от его блока управления. В этом случае параллелизм (***параллелизм на уровне инструкций***) получается путем выполнения нескольких операций над одним и тем же фрагментом данных. Типы задач, которые могут быть эффективно решены в этой архитектуре, довольно специфичны, например, шифрование данных; по этой причине архитектура **MISD** удел скорее специализированного оборудования, чем широко применяемая практическая конфигурация вычислительных систем.\n",
    "\n",
    "#### Архитектура **SIMD**\n",
    "\n",
    "**SIMD**-компьютер состоит из **N** одинаковых процессоров, каждый со своей собственной локальной памятью. Все процессоры работают под управлением одного потока команд; в дополнение к этому, есть **N** потоков данных, по одному для каждого процессора. Процессоры работают ***одновременно*** на каждом шаге и выполняют ***одну*** и ту же инструкцию, но ***на разных*** элементах данных. Это пример параллелизма на уровне данных. Архитектуры **SIMD** гораздо более универсальны, чем архитектуры **MISD**. C помощью параллельных алгоритмов на **SIMD**-компьютерах могут быть решены mногочисленные задачи, охватывающие широкий спектр приложений. Еще одна интересная особенность заключается в том, что алгоритмы для этих компьютеров относительно просты в проектировании, анализе и реализации. Ограничение состоит в том, что на **SIMD**-архитектуре могут быть решены только задачи, которые могут быть разделены на несколько одинаковых подзадач, которые будут решаться одновременно, одним и тем же набором инструкций. Современные **GPU**-вычислители построены с использованием встроенных модулей **SIMD**, что привело к широкому использованию этой вычислительной парадигмы.\n",
    "\n",
    "#### Архитектура **МIMD**\n",
    "\n",
    "Этот класс параллельных архитектур является самым общим и более мощным классом в соответствии с классификацией Флинна. В архитектуре **МIMD** реализовано **N** процессоров, **N** потоков команд и **N** потоков данных. Каждый процессор имеет свой собственный блок управления и локальную память, что делает архитектуры **MIMD** более мощными в вычислительном отношении, чем те, которые используются в **SIMD**. ***Каждый процессор*** работает под управлением ***потока инструкций***, выдаваемых его ***собственным блоком управления***; поэтому процессоры могут потенциально запускать ***разные программы на разных данных***, решая различные подзадачи, которые могут быть частью одной большой задачи. Высокая производительность в **MIMD**-архитектуре достигается с помощью уровня параллелизма с **потоками** и/или **процессами**. Это также означает, что процессоры обычно работают ***асинхронно***. В настоящее время эта архитектура применяется во многих компьютерах, суперкомпьютерах и вычислительных компьютерных сетях. Однако нужно учитывать, что асинхронные алгоритмы сложно проектировать, анализировать и реализовывать.\n",
    "\n",
    "#### Современные гетерогенные архитектуры\n",
    "\n",
    "Появление **GPU**-вычислителей изменило характер того, как строятся, используются и программируются высокопроизводительные вычислительные системы и суперкомпьютеры. Несмотря на высокую производительность, предлагаемую графическими процессорами, они не могут считаться автономным процессором, так как они всегда должны сопровождаться обычными процессорами и блоками памяти. Поэтому парадигма программирования выглядит так:\n",
    "***центральный процессор*** управляет основным потоком программы ***последовательным образом***, передавая ***графическому ускорителю*** задачи, которые являются вычислительно ***дорогостоящими*** и имеют ***высокую степень параллелизма***. Связь между процессором и графическим процессором может происходить не только за счет использования высокоскоростной шины, но и путем совместного использования одной области памяти. Фактически, в случае, когда оба устройства не имеют собственных областей памяти, можно обратиться к общей области памяти, используя библиотеки программного обеспечения, предоставляемые различными моделями программирования, такими как **CUDA** и **OpenCL**. Эти архитектуры называются ***гетерогенными архитектурами***, в которых приложения могут создавать структуры данных в одном адресном пространстве и отправлять задание на аппаратное обеспечение **GPU**-устройства. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p style=\"text-align: center; font-size: 60pt; font-weight: bold; color: rgb(0, 0, 190); padding: 4px; margin-top: 3%; line-height: 150%;\">Введение в технологии параллельного программирования</p>\n",
    "<p style=\"text-align: center; font-size: 72pt; font-weight: bold; color: rgb(0, 0, 190); padding: 10px; margin-top: 3%; line-height: 150%;\">OpenMP <font style=\"font-size: 48pt; font-weight: bold; color: rgb(0, 0, 190); line-height: 150%;\">и</font> MPI</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p style=\"text-align: center; font-size: 36pt; font-weight: bold; color: rgb(0, 0, 190); padding: 40px; margin-top: 3%; line-height: 150%;\">Параллельное программирование систем с общей памятью средствами OpenMP</p>\n",
    "\n",
    "Технология **OpenMP** является одним из наиболее популярных средств параллельного программирования для вычислительных систем с общей памятью. Концептуально технология **OpenMP** основана на использовании традиционных последовательных языков программирования, таких как **FORTRAN** и **С/С++**, и специализированных директив компилятора, управляющих библиотеками параллельной реализации алгоритмов. \n",
    "\n",
    "Для создания параллельной версии, из программы последовательной реализации вычислительного алгоритма, программисту предоставляется набор директив, функций и переменных окружения. Такой подход позволяет разрабатывать один и тот же вариант программы как для параллельного, так и последовательного исполнения.\n",
    "\n",
    "В технологии **OpenMP** параллелизм реализуется на основе ***многопоточности***. При запуске программы создается один управляющий ***\"master\"***-поток, который сам создает набор исполнительных ***\"slave\"***-потоков. Вычислительная нагрузка перераспределяется между всеми потоками, причем созданные потоки могут конкурировать между собой за процессорные ресурсы.\n",
    "\n",
    "Существенным достоинством технологии **OpenMP** является возможность поэтапного распараллеливания больших программ, помере нахождения участков с потенциальным параллелизмом. Это значительно облегчает процесс адаптации последовательных программ к параллельным вычислительным системам, а также отладку и оптимизацию программ. Программы могут содержать любое количество параллельных и последовательных участков, причем параллельные участки могут быть вложенными друг в друга.\n",
    "\n",
    "Активным стандартом **OpenMP** является стандарт **4.5**, принятый в 2015 году. В июле 2018 начато активной обсуждение новой версии стандарта **OpenMP 5.0**.\n",
    "\n",
    "Программа, использующая технологию **OpenMP**, может содержать следующие элементы:\n",
    "- ***директивы компилятора*** - создают потоки, распределяют нагрузку между потоками и обеспечивают их синхронизацию\n",
    "- ***подпрограммы библиотеки времени выполнения*** - используются для установки и обработки атрибутов потоков\n",
    "- ***переменные окружения*** - позволяют управлять поведением параллельных программ\n",
    "\n",
    "\n",
    "### Переменные\n",
    "\n",
    "В **OpenMP** переменные в параллельных областях программы разделяются на два основных класса:\n",
    "- ***private*** (локальные, приватные; каждая нить видит свой экземпляр данной переменной).\n",
    "- ***shared*** (общие; все нити видят одну и ту же переменную);\n",
    "\n",
    "По умолчанию, все переменные, порождённые ***вне*** параллельной области, ***при входе*** в неё остаются ***общими***. Исключение составляют переменные, являющиеся счетчиками итераций в цикле. Переменные, порождённые ***внутри*** параллельной области, по умолчанию являются ***локальными***.\n",
    "\n",
    "### Директивы\n",
    "\n",
    "Директивы **OpenMP** в языке **С/С++** задаются указаниями препроцессору, начинающимися с \n",
    "\n",
    "***#pragma omp***\n",
    "\n",
    "***#pragma omp directive-name \n",
    "[опция[[,] опция]...]***\n",
    "\n",
    "Объектом действия большинства директив является один оператор или блок, перед которым расположена директива в исходном тексте программы.\n",
    "\n",
    "Чтобы задействовать функции библиотеки **OpenMP** периода выполнения (исполняющей среды), в программу нужно включить заголовочный файл ***omp.h*** (для программ на языке **FORTRAN** – файл ***omp_lib.h*** или модуль ***omp_lib***).\n",
    "\n",
    "Также нужно задать количество нитей, выполняющих параллельные области программы, определив значение переменной среды **OMP_NUM_THREADS**:\n",
    "\n",
    "**export OMP_NUM_THREADS=n**\n",
    "\n",
    "### Параллельные и последовательные области\n",
    "\n",
    "Параллельные области задаются директивой\n",
    "\n",
    "**#pragma omp parallel [опция[[,] опция]...]**\n",
    "\n",
    "Данная директива порождает **OMP_NUM_THREADS-1** нитей, каждая нить получает свой уникальный номер, причём порождающая нить получает номер 0 и становится основной нитью группы («master»-нитью). При выходе из параллельной области производится неявная синхронизация и уничтожаются все нити, кроме породившей.\n",
    "\n",
    "### Пример № 1: Параллельная область \n",
    "```c++\n",
    "#include <stdio.h>\n",
    "int main(int argc, char *argv[])\n",
    "{\n",
    "printf(\"Последовательная область 1\\n\");\n",
    "#pragma omp parallel\n",
    "{\n",
    "printf(\"Параллельная область\\n\");\n",
    "}\n",
    "printf(\"Последовательная область 2\\n\");\n",
    "}\n",
    "```\n",
    "Сохраним этот код в файл ***OMP_Hello.cpp***, скомпилируем и выполним\n",
    "\n",
    "#### На локальной машине\n",
    "Компиляцию этой программы проводим командой\n",
    "```bash\n",
    "g++ -fopenmp OMP_Hello.cрр -o omp_hello\n",
    "```\n",
    "После компиляции в терминале вводим команду\n",
    "```bash\n",
    "export OMP_NUM_THREADS=12\n",
    "```\n",
    "Затем в терминале вводим команду\n",
    "```bash\n",
    "./omp_hello\n",
    "```\n",
    "### Функции OpenMP для работы с временем\n",
    "Функции для работы с системным таймером:\n",
    "```c++\n",
    "double omp_get_wtime(void);\n",
    "```\n",
    "Возвращает астрономическое время в секундах, прошедшее с некоторого момента в прошлом.\n",
    "```c++\n",
    "double omp_get_wtick(void);\n",
    "```\n",
    "Возвращает в вызвавшей нити разрешение таймера в секундах.\n",
    "\n",
    "### Некоторые функции OpenMP для параллельной работы\n",
    "\n",
    "**reduction(оператор:список)** – задаёт оператор и список общих переменных. Для каждой переменной создаются локальные копии в каждой нити и инициализируются соответственно типу оператора (для аддитивных – 0 или аналоги, для мультипликативных – 1 или аналоги). После выполнения всех операторов параллельной области выполняется заданный **оператор**; **оператор** - это (для языка **С/С++**):  **+, *, -, &, |, ^, &&, ||**. Порядок выполнения операторов не определён, поэтому результат может отличаться от запуска к запуску.\n",
    "\n",
    "**omp_in_parallel()** – возвращает 1, если она была вызвана из активной параллельной области программы.\n",
    "\n",
    "### Синхронизация в OpenMP\n",
    "\n",
    "Самый распространенный способ синхронизации в **OpenMP** – ***барьер***. Он оформляется с помощью директивы **barrier**.\n",
    "```c++\n",
    "#pragma omp barrier\n",
    "```\n",
    "Нити, выполняющие текущую параллельную область, дойдя до этой директивы, останавливаются и ждут, пока все нити не дойдут до этой точки программы, после чего разблокируются и продолжают работать дальше. Кроме того, для ***разблокировки*** необходимо, чтобы ***все*** синхронизируемые ***нити*** завершили ***все*** порождённые ими ***задачи (task)***.\n",
    "\n",
    "### Пример №2: Перемножение двух квадратных матриц\n",
    "\n",
    "```c++\n",
    "#include <stdio.h>\n",
    "#include <omp.h>\n",
    "#define N 1024\n",
    "#define NTeams 32\n",
    "#define NThreads N/NTeams\n",
    "\n",
    "\n",
    "double a[N][N], b[N][N], c[N][N];\n",
    "\n",
    "int main()\n",
    "{\n",
    " int i, j, k;\n",
    " double t1, t2, t3, t4, t5, t6, t7, t8, t9, t10, t11, t12, t13, t14, t15, t16;\n",
    " // инициализация матриц\n",
    " for (i=0; i<N; i++){\n",
    "     for (j=0; j<N; j++){\n",
    "         a[i][j]=b[i][j]=i*j;\n",
    "         c[i][j]=0.0;\n",
    "     }\n",
    " }\n",
    "\n",
    " t1=omp_get_wtime();\n",
    " // основной вычислительный блок IJK\n",
    " for(i=0; i<N; i++){\n",
    "          for(j=0; j<N; j++){\n",
    "                    for(k=0; k<N; k++){\n",
    "                            c[i][j]+=a[i][k]*b[k][j];\n",
    "                   }\n",
    "          }\n",
    " }\n",
    " t2=omp_get_wtime();\n",
    " printf(\"Serial IJK Time= %lf sec\\n\", t2-t1);\n",
    "\n",
    " t3=omp_get_wtime();\n",
    " // основной вычислительный блок KIJ\n",
    " for(k=0; k<N; k++){\n",
    "          for(i=0; i<N; i++){\n",
    "                   for(j=0; j<N; j++){\n",
    "                            c[i][j]+=a[i][k]*b[k][j];\n",
    "                   }\n",
    "          }\n",
    " }\n",
    " t4=omp_get_wtime();\n",
    " printf(\"Serial KIJ Time= %lf sec\\n\", t4-t3);\n",
    "\n",
    " t5=omp_get_wtime();\n",
    " // основной вычислительный блок IJK\n",
    " #pragma omp parallel for shared(a, b, c) private(i, j, k)\n",
    " for(i=0; i<N; i++){\n",
    "          for(j=0; j<N; j++){\n",
    "                   for(k=0; k<N; k++){\n",
    "                            c[i][j]+=a[i][k]*b[k][j];\n",
    "                   }\n",
    "          }\n",
    " }\n",
    " t6=omp_get_wtime();\n",
    " printf(\"OpenMP IJK Time= %lf sec\\n\", t6-t5);\n",
    "\n",
    " t7=omp_get_wtime();\n",
    " // основной вычислительный блок KIJ\n",
    " #pragma omp parallel for shared(a, b, c) private(i, j, k)\n",
    " for(k=0; k<N; k++){\n",
    "          for(i=0; i<N; i++){\n",
    "                   for(j=0; j<N; j++){\n",
    "                            c[i][j]+=a[i][k]*b[k][j];\n",
    "                   }\n",
    "          }\n",
    " }\n",
    " t8=omp_get_wtime();\n",
    " printf(\"OpenMP KIJ Time= %lf sec\\n\", t8-t7);\n",
    "\n",
    " t9=omp_get_wtime();\n",
    " // основной вычислительный блок IJK\n",
    " #pragma omp target map(to: a,b) map(from: c)\n",
    " #pragma omp parallel for private(i,j,k)\n",
    " for(i=0; i<N; i++){\n",
    "          for(j=0; j<N; j++){\n",
    "                   for(k=0; k<N; k++){\n",
    "                            c[i][j]+=a[i][k]*b[k][j];\n",
    "                   }\n",
    "          }\n",
    " }\n",
    " t10=omp_get_wtime();\n",
    " printf(\"OpenMP Naive GPU IJK Time= %lf sec\\n\", t10-t9);\n",
    "\n",
    " t11=omp_get_wtime();\n",
    " // основной вычислительный блок IJK\n",
    " #pragma omp target map(to: a,b) map(from: c)\n",
    " #pragma omp parallel for private(i,j)\n",
    " for(k=0; k<N; k++){\n",
    "          for(i=0; i<N; i++){\n",
    "                   for(j=0; j<N; j++){\n",
    "                            c[i][j]+=a[i][k]*b[k][j];\n",
    "                   }\n",
    "          }\n",
    " }\n",
    "\n",
    " t12=omp_get_wtime();\n",
    " printf(\"OpenMP Naive GPU KIJ Time= %lf sec\\n\", t12-t11);\n",
    "\n",
    " t13=omp_get_wtime();\n",
    " // основной вычислительный блок IJK\n",
    " #pragma omp target data map(to: a,b) map(tofrom: c)\n",
    " {\n",
    " #pragma omp target teams num_teams(NTeams) thread_limit(NThreads)\n",
    " #pragma omp parallel for private(j,k)\n",
    " for(i=0; i<N; i++){\n",
    "          for(j=0; j<N; j++){\n",
    "                   for(k=0; k<N; k++){\n",
    "                            c[i][j]+=a[i][k]*b[k][j];\n",
    "                   }\n",
    "          }\n",
    " }\n",
    " }\n",
    " t14=omp_get_wtime();\n",
    " printf(\"OpenMP Teams GPU IJK Time= %lf sec\\n\", t14-t13);\n",
    "\n",
    " t15=omp_get_wtime();\n",
    " // основной вычислительный блок IJK\n",
    " #pragma omp target data map(to: a,b) map(tofrom: c)\n",
    " {\n",
    " #pragma omp target teams num_teams(NTeams) thread_limit(NThreads)\n",
    " #pragma omp parallel for private(i,j)\n",
    " for(k=0; k<N; k++){\n",
    "          for(i=0; i<N; i++){\n",
    "                   for(j=0; j<N; j++){\n",
    "                            c[i][j]+=a[i][k]*b[k][j];\n",
    "                   }\n",
    "          }\n",
    " }\n",
    " }\n",
    " t16=omp_get_wtime();\n",
    " printf(\"OpenMP Teams GPU KIJ Time= %lf sec\\n\", t16-t15);\n",
    "\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p style=\"text-align: center; font-size: 36pt; font-weight: bold; color: rgb(0, 0, 190); padding: 40px; margin-top: 3%; line-height: 150%;\">Параллельное программирование систем с распределенной памятью средствами MPI</p>\n",
    "\n",
    "***Message Passing Interface*** (**MPI**, интерфейс передачи сообщений) — стандарт программного интерфейса для передачи информации, который позволяет обмениваться сообщениями между процессами, выполняющими одну задачу.\n",
    "\n",
    "**MPI** является наиболее распространённым стандартом интерфейса обмена данными в параллельном программировании, существуют его реализации для большого числа компьютерных платформ. Используется при разработке программ для кластеров и суперкомпьютеров. Основным средством коммуникации между процессами в **MPI** является передача сообщений друг другу. \n",
    "\n",
    "### Параллелизм в MPI\n",
    "\n",
    "Цели распараллеливания:\n",
    "- равномерная загрузка вычислителей\n",
    "- минимизация количества и объёма необходимых пересылок данных\n",
    "\n",
    "При построении параллельного вычислительного процесса необходимо выделить группы операций, которые могут вычисляться независимо и одновременно. Возможность этого определяется наличием или отсутствием в программе истинных ***информационных зависимостей***. Две операции программы называются ***информационно зависимыми***, если ***результат*** выполнения ***одной*** операции используется в качестве ***аргумента*** в ***другой***. Пересылка данных требуется, если есть информационная зависимость между операциями, которые при выбранной схеме распределения попадают на разные вычислителей. Стандарт **MPI** 3.1 (4 июня 2015 года), содержит более 450 процедур. Стандартные функции начинаются с префикса **MPI_**.\n",
    "\n",
    "### Основные термины MPI\n",
    "\n",
    "- ***Процессы*** - обрабатывают данные и посылают сообщения\n",
    "- ***Сообщение*** – массив однотипных данных\n",
    "- ***Группа*** – упорядоченное множество процессов\n",
    "- ***Коммуникатор*** – контекст обмена группы\n",
    "\n",
    "В языке **С/С++** коммуникаторы имеют предопределённый тип **MPI_Comm**.\n",
    "\n",
    "##### Виды коммуникаторов\n",
    "\n",
    "- **MPI_COMM_WORLD** – коммуникатор для всех процессов приложения\n",
    "- **MPI_COMM_SELF** – коммуникатор, включающий только текущий процесс\n",
    "- **MPI_COMM_NULL** – коммуникатор, не содержащий ни одного процесса\n",
    "\n",
    "Каждый процесс может одновременно входить в разные коммуникаторы. Два основных атрибута процесса: **коммуникатор (группа)** и **номер процесса** в коммуникаторе (группе). Если коммуникатор содержит ***n*** процессов, то номера процессов в нём лежат в пределах от ***0*** до ***n–1***.\n",
    "\n",
    "- ***Сообщение*** — набор данных некоторого типа.\n",
    "\n",
    "Атрибуты сообщения: номер процесса-отправителя, номер процесса-получателя, идентификатор сообщения, коммуниктор.\n",
    "\n",
    "- ***Идентификатор сообщения (тег)*** - целое неотрицательное число в диапазоне от **0** до **MPI_TAG_UB**\n",
    "\n",
    "Для работы с атрибутами сообщений введена структура **MPI_Status**. \n",
    "\n",
    "### Некоторые процедуры MPI\n",
    "\n",
    "***Инициализация параллельной части*** программы.\n",
    "```c++\n",
    "int MPI_Init(int *argc, char ***argv)\n",
    "```\n",
    "Почти все другие процедуры **MPI** могут быть вызваны только после вызова **MPI_Init**. Инициализация параллельной части для каждого приложения должна выполняться только один раз.\n",
    "\n",
    "***Завершение параллельной части*** программы.\n",
    "```c++\n",
    "int MPI_Finalize(void)\n",
    "```\n",
    "Все последующие обращения к большинству процедур **MPI**, в том числе к **MPI_Init**, запрещены. К моменту вызова **MPI_Finalize** каждым процессом программы все действия, требующие его участия в обмене сообщениями, должны быть завершены. \n",
    "```c++\n",
    "int MPI_Initialized(int *flag)\n",
    "```\n",
    "В аргументе ***flag*** возвращает 1, если вызвана после процедуры **MPI_Init**, и 0 - в противном случае.\n",
    "```c++\n",
    "int MPI_Finalized(int *flag)\n",
    "```    \n",
    "В аргументе ***flag*** возвращает 1, если вызвана после процедуры MPI_Finalize , и 0 - в противном случае.\n",
    "Эти процедуры можно вызвать до **MPI_Init** и после **MPI_Finalize**.\n",
    "```c++\n",
    "int MPI_Comm_size(MPI_Comm comm, int *size)\n",
    "```\n",
    "В аргументе ***size*** возвращает число параллельных процессов в коммуникаторе comm.\n",
    "```c++\n",
    "int MPI_Comm_rank(MPI_Comm comm, int *rank)\n",
    "```\n",
    "В аргументе ***rank*** возвращает номер процесса в коммуникаторе comm в диапазоне от ***0*** до ***size-1***.\n",
    "```c++\n",
    "double MPI_Wtime(void)\n",
    "```\n",
    "Возвращает для каждого вызвавшего процесса астрономическое время в секундах (вещественное число двойной точности), прошедшее с некоторого момента в прошлом. Момент времени, используемый в качестве точки отсчёта, не будет изменён за время существования процесса.\n",
    "```c++\n",
    "double MPI_Wtick(void)\n",
    "```\n",
    "Возвращает разрешение таймера в секундах. Таймеры разных процессов могут быть не синхронизированы и выдавать существенно различающиеся значения, это можно определить по значению предопределенной константы **MPI_WTIME_IS_GLOBAL**: ***1*** – синхронизированы, ***0*** – нет.\n",
    "```c++\n",
    "int MPI_Get_processor_name(char *name, int *len)\n",
    "```\n",
    "Возвращает в строке name имя узла, на котором запущен вызвавший процесс. В переменной ***len*** возвращается количество символов в имени, не превышающее константы **MPI_MAX_PROCESSOR_NAME**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Параллельное программирование в Python \n",
    "\n",
    "Платформа **Python** включает в себя ряд механизмов, реализующих параллельные технологии, такие как **``threading``**, **``queues``** или **``multiprocessing``**. Долгое время модуль **``threading``** использовался как основной способ достижения параллельности. Некоторое время назад, модуль **``multiprocessing``** был добавлен в пакет стандартных библиотек **Python**. \n",
    "\n",
    "Процесс (**process**) - это исполняемый экземпляр приложения. Нить (**thread**) представляет собой активный поток управления, который может быть активирован параллельно с другими потоками в рамках одного и того же процесса. Каждый поток может выполнять набор инструкций (обычно, функцию) независимо и параллельно с другими процессами или потоками. Однако, будучи разными активными потоками в одном и том же процессе, они совместно используют адресацию и  структуры данных. Поток иногда называют легким процессом, поскольку он обладает многими характеристиками процесса, т.е. реализация потока менее обременительна, чем реализация реального процесса. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p style=\"text-align: center; font-size: 36pt; font-weight: bold; color: rgb(0, 0, 190); padding: 40px; margin-top: 3%; line-height: 150%;\">Настройка работы **Jupyter** для использования параллельных технологий: пакеты IPyparallel, MPI4Py и др.</p>\n",
    "\n",
    "Пакет **IPyparallel** предоставляет набор инструментовдля организации параллельных вычислительных процессов. Кластер организуемый по-умолчанию обеспечивает распераллеливание на основе технологий ***threading***, ***queues*** и ***multiprocessing***. Однако предеоставляются возможности организации вычислительных кластеров на основе других технологий параллельного исполнения вычислительных процессов.\n",
    "\n",
    "Для инициализации вкладки ***IPython Clusters*** в **Jupyter** необходимо выполнить команду\n",
    "```bash\n",
    "ipcluster nbextension enable [--user]\n",
    "```\n",
    "Для отключения\n",
    "```bash\n",
    "ipcluster nbextension disable [--user]\n",
    "```\n",
    "\n",
    "Для построения кластера на основе технологии **MPI** необходимо сгенерировать шаблон профиля, для этого в терминале выполняем команду\n",
    "```bash\n",
    "ipython profile create --parallel --profile=mpi\n",
    "```\n",
    "В папке с файлами профиля необходимо отредактироавть файл ***IPYTHONDIR/profile_mpi/ipcluster_config.py***, вставив\n",
    "в конец строку\n",
    "```bash\n",
    "c.IPClusterEngines.engine_launcher_class = 'MPIEngineSetLauncher'\n",
    "```\n",
    "Для запуска **MPI**-кластера  в терминале, выполняем команду\n",
    "```bash\n",
    "ipcluster start -n 4 --profile=mpi\n",
    "```\n",
    "\n",
    "Для начала использования организованного вычислительного кластера в ячейках **Jupyter**, необходимо импортировать пакет **IPyparallel**\n",
    "```python\n",
    "import ipyparallel\n",
    "```\n",
    "В следующей ячейке создаем экземпляр клиента и объекта доступа к свойствам и методам.\n",
    "- для кластера по-умолчанию\n",
    "```python\n",
    "client = ipyparallel.Client()\n",
    "dview = client[:]\n",
    "```\n",
    "- для **MPI**-кластера\n",
    "```python\n",
    "client = ipyparallel.Client(profile='mpi')\n",
    "dview = client[:]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример № 1 : Построение фрактального множества Жюлиа\n",
    "\n",
    "***Множество Жюлиа*** функции $\\;f$ обозначаемое $J\\left( f \\right)$ определяется как\n",
    "$$\n",
    "J\\left( f \\right)=\\partial \\left\\{ z:f^{(n)} \\rightarrow \\infty \\;\\; \\text{при} \\;\\; n \\rightarrow \\infty \\right\\}\n",
    "$$\n",
    "т.е. множество Жюлиа функции $\\;f$ - это граница множества точек $\\;z$, стремящихся к бесконечности при итерировании $f\\left( z \\right)$. Множество названо в честь французского математика Гастона Жюлиа (1893 - 1978).\n",
    "\n",
    "Рассмотрим функцию $\\; f:\\mathbb{C} \\rightarrow \\mathbb{C}$ такую, что $f\\left( z \\right)=z^2 + c$, где $c \\in \\mathbb{C}$. Итерационный процесс выглядит следующим образом\n",
    "$$\n",
    "z_{n} = f\\left( z_{n-1} \\right) = . . . = f^{n}\\left( z_{0} \\right)\n",
    "$$\n",
    "Такая динамическая система ведет себя хатически практичеси при любых $c$,т.е. две близкие начальные точки $z_{0} \\; и \\; z_{0}+\\varepsilon $ дают сильно расходящиеся траектории при больших $n$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named LSA_NBody",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-133cdeb31934>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mipyparallel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mLSA_NBody\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mNB\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'matplotlib inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named LSA_NBody"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import ipyparallel\n",
    "import numpy as np\n",
    "import LSA_NBody as NB\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Создаем объект доступа к запущенному кластеру\n",
    "client = ipyparallel.Client() # Default-Cluster\n",
    "#client = ipyparallel.Client(profile='mpi') # MPI-Cluster\n",
    "# Создаем объект доступа и управления параллельными исполнителями\n",
    "dview = client[:]\n",
    "# Указываем необходимость синхронизации\n",
    "dview.block = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Исследуемая функция\n",
    "def f(z, c=-0.065+0.66j):\n",
    "    return z**2 + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Исходные данные\n",
    "# n x m - разрешение исследования плоскости\n",
    "n = 1024\n",
    "m = 1024\n",
    "# zmin, zmax - ограничения комплексной области\n",
    "zmin = -1.3 - 1j * 1.3\n",
    "zmax = 1.3 + 1j * 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Наборы точек на осях для построения сетки\n",
    "xs = np.linspace(zmin.real, zmax.real, n)\n",
    "ys = np.linspace(zmin.imag, zmax.imag, m)\n",
    "# Построение сетки\n",
    "X, Y = np.meshgrid(xs, ys)\n",
    "Z = X + 1j * Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построение множества Жюлиа в \"чистом\" виде весьма затруднительно практически, поэтому мы будем искусственно обрывать итерации по достижении некоторой границы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Функция построения множества Жюлиа функции f(z)\n",
    "def julia(f, Z, tmax=512):\n",
    "    J = np.ones(Z.shape) * tmax\n",
    "    for t in xrange(tmax):\n",
    "        mask = np.abs(Z) <= 2.\n",
    "        Z[ mask] = f(Z[mask])\n",
    "        J[~mask] -= 1\n",
    "    return J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ZZ = np.copy(Z)\n",
    "J = julia(f, ZZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "figure(figsize=[10,10])\n",
    "plt.imshow(J)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем пакет **NumPy** и необходимые функции для каждого параллельного испольнителя, используя магическую команду **jupyter** **``%%px``**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def f(z, c=-0.065+0.66j):\n",
    "    return z**2 + c\n",
    "\n",
    "def julia_parallel(f,Z,tmax=512):\n",
    "    J = np.ones(Z.shape) * tmax\n",
    "    for t in xrange(tmax):\n",
    "        mask = np.abs(Z) <= 2.\n",
    "        Z[mask] = f(Z[mask])\n",
    "        J[~mask] -= 1\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# распределяем набор данных Z по всем параллельным испольнителям\n",
    "dview.scatter('Z', Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# На каждом параллельном исполнителе создаем копию набора данных\n",
    "%px ZZ = np.copy(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# На каждом параллельном исполнителе строим множество Жюлиа функции f(z) для участка плоскости\n",
    "%px J = julia_parallel(f,ZZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Собираем результат с каждого параллельного исполнителя\n",
    "PJ = dview.gather('J')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "figure(figsize=[10,10])\n",
    "plt.imshow(PJ)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы провели построение множества Жюлиа двумя способами: последовательным и параллельным. Давайте сравним время исполнения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z = X + 1j * Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "J = julia(f, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z = X + 1j * Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "dview.scatter('Z', Z)\n",
    "%px ZZ = np.copy(Z)\n",
    "%px J = julia_parallel(f,ZZ)\n",
    "PJ = dview.gather('J')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример № 2 : Перемножение квадратных матриц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import ipyparallel\n",
    "import numpy as np\n",
    "import os\n",
    "import LSA_NBody as NB\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# настройка отображения чисел и массивов\n",
    "np.set_printoptions(precision=2)\n",
    "np.set_printoptions(threshold=np.nan, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Создаем объект доступа к запущенному кластеру\n",
    "client = ipyparallel.Client() # Default-Cluster\n",
    "#client = ipyparallel.Client(profile='mpi') # MPI-Cluster\n",
    "# Создаем объект доступа и управления параллельными исполнителями\n",
    "dview = client[:]\n",
    "# Указываем необходимость синхронизации\n",
    "dview.block = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MatMul(a,b):\n",
    "    c = np.zeros_like(a)\n",
    "    for i in range(np.size(a,axis=0)):\n",
    "        for j in range(np.size(a,axis=0)):\n",
    "                   for k in range(np.size(a,axis=0)):\n",
    "                            c[i,j]+=a[i,k]*b[k,j]\n",
    "                            \n",
    "    return c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 20\n",
    "a = np.matrix(np.random.random((N,N)))\n",
    "b = np.matrix(np.random.random((N,N)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "c = a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "c1 = MatMul(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c-c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%px\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def MatMul(a,b):\n",
    "    c = np.zeros_like(a)\n",
    "    for i in range(np.size(a,axis=0)):\n",
    "        for j in range(np.size(a,axis=0)):\n",
    "                   for k in range(np.size(a,axis=0)):\n",
    "                            c[i,j]+=a[i,k]*b[k,j]\n",
    "                            \n",
    "    return c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print client.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dview.scatter('id', client.ids, flatten=True)\n",
    "dview.scatter('a', a)\n",
    "dview.scatter('b', b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%px c2 = MatMul(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c2 = dview.gather(\"c2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c - c2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получено неверное решение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import ipyparallel\n",
    "import numpy as np\n",
    "import os\n",
    "import LSA_NBody as NB\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# настройка отображения чисел и массивов\n",
    "np.set_printoptions(precision=2)\n",
    "np.set_printoptions(threshold=np.nan, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Создаем объект доступа к запущенному кластеру\n",
    "client = ipyparallel.Client() # Default-Cluster\n",
    "#client = ipyparallel.Client(profile='mpi') # MPI-Cluster\n",
    "# Создаем объект доступа и управления параллельными исполнителями\n",
    "dview = client[:]\n",
    "# Указываем необходимость синхронизации\n",
    "dview.block = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 2000\n",
    "a = np.matrix(np.random.random((N,N)))\n",
    "b = np.matrix(np.random.random((N,N)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "c = a*b\n",
    "#c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dview.scatter('id', client.ids, flatten=True)\n",
    "dview.scatter('a', a)\n",
    "dview.push({'b':b})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%px\n",
    "\n",
    "#print id\n",
    "c1 = a*b\n",
    "#print c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c1 = dview.gather('c1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.allclose(c1,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ПРИМЕР: Умножение матрицы на вектор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Генерируем исходные данные\n",
    "N = 16\n",
    "A = np.random.rand(N, N)\n",
    "np.save(\"random-matrix.npy\", A)\n",
    "x = np.random.rand(N)\n",
    "np.save(\"random-vector.npy\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mpi-matrix-vector.py\n"
     ]
    }
   ],
   "source": [
    "%%file mpi-matrix-vector.py\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "p = comm.Get_size()\n",
    "\n",
    "def matvec(comm, A, x):\n",
    "    m = A.shape[0] / p\n",
    "    y_part = np.dot(A[rank * m:(rank+1)*m], x)\n",
    "    y = np.zeros_like(x)\n",
    "    comm.Allgather([y_part,  MPI.DOUBLE], [y, MPI.DOUBLE])\n",
    "    return y\n",
    "\n",
    "A = np.load(\"random-matrix.npy\")\n",
    "x = np.load(\"random-vector.npy\")\n",
    "y_mpi = matvec(comm, A, x)\n",
    "\n",
    "if rank == 0:\n",
    "    y = np.dot(A, x)\n",
    "    print(y_mpi)\n",
    "    print \"sum(y - y_mpi) =\", (y - y_mpi).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.63099804 3.60185086 4.32829501 5.03281235 4.87904132 4.05513575\r\n",
      " 5.77075411 3.47516388 3.65155894 4.41091892 3.85286181 4.9245572\r\n",
      " 4.83204844 5.86654776 4.32337778 4.07026184]\r\n",
      "sum(y - y_mpi) = 0.0\r\n"
     ]
    }
   ],
   "source": [
    "!mpirun -n 4 python mpi-matrix-vector.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "<center>\n",
    "<table style=\"background: transparent; text-align: center; font-size: 36pt; font-weight: bold; margin-top: -5%;\">\n",
    "<tr >\n",
    "<td><img src=\"Images/numpy_logo.png\" width=\"180\" /><b>NumPy</b></td>\n",
    "<td><center><img src=\"Images/scipy_logo.png\" width=\"180\" /><b>SciPy</b></td>\n",
    "<td><center><img src=\"Images/SymPy_logo.jpg\" width=\"180\" /></td>\n",
    "<td><center><img src=\"Images/Pythonchik_2.png\" width=\"180\" /></td>\n",
    "</tr>\n",
    "</table>\n",
    "</center>\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "livereveal": {
   "autolaunch": true,
   "height": 800,
   "scroll": true,
   "theme": "sky",
   "width": 1200
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
